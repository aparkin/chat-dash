"""
ParseTEI.py

This script is designed to parse TEI XML files generated by the Grobid service, which processes PDF documents to extract structured information. It is intended to be used after running ProcessPDFsWithGrobid.py, which converts PDF files into TEI XML format.

In addition to parsing TEI XML files, this script incorporates named entity recognition (NER) using BERT models. The current models are configured to extract entities related to Genes, Bioprocesses, Chemicals, and Organisms. This enhances the extracted data with additional semantic information, making it more valuable for downstream analysis.

The primary function of ParseTEI.py is to extract key information from the TEI XML files, such as titles, authors, affiliations, abstracts, references, body sections, funding information, tables, figures, and publication details. The extracted data, along with the NER results, is then saved in a structured JSON format for further analysis or integration into other systems.

Usage:
    python ParseTEI.py <output_directory> <file_paths> [--workers <number_of_workers>]

Parameters:
    output_directory: The directory where the output JSON files will be saved.
    file_paths: Paths to the TEI XML files to be processed. If not provided, file paths will be read from standard input.
    --workers: (Optional) The number of worker processes to use for parallel file processing. Default is 1.

This script is part of a two-step process:
1. ProcessPDFsWithGrobid.py: Converts PDF files into TEI XML format using the Grobid service.
2. ParseTEI.py: Parses the TEI XML files to extract and structure the information into JSON format, with additional NER processing.

Example usage: (running from the directory where the Grobid output is)

ls *.xml | python3 ../../utilities/ParseTEI.py output --workers 10

Ensure that the Grobid server is running and accessible before using these scripts. The Grobid server can be set up using Docker, as described in the ProcessPDFsWithGrobid.py script.
"""

import os
import sys
import json
import argparse
import concurrent.futures
import logging
import warnings
from typing import List, Dict, Any
from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline
from lxml import etree
from tqdm import tqdm
from contextlib import redirect_stdout, redirect_stderr
from io import StringIO

# Set logging level to ERROR to suppress warnings and info messages
logging.basicConfig(level=logging.ERROR)

# Suppress specific warnings
warnings.filterwarnings("ignore", message="Device set to use mps:0")
warnings.filterwarnings("ignore", message="Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.")

class NERPipeline:
    def __init__(self, model_name: str):
        # Suppress output during initialization
        f = StringIO()
        with redirect_stdout(f), redirect_stderr(f):
            self.tokenizer = AutoTokenizer.from_pretrained(model_name, model_max_length=512)
            self.model = AutoModelForTokenClassification.from_pretrained(model_name)
            self.ner_pipeline = pipeline("ner", model=self.model, tokenizer=self.tokenizer, aggregation_strategy="simple")

    def process_text(self, text: str, max_length: int = 512, overlap: int = 50) -> List[Dict[str, Any]]:
        stride = max_length - overlap
        ner_results = []
        for i in range(0, len(text), stride):
            chunk = text[i:i + max_length]
            if len(chunk) < max_length:
                break
            chunk_results = self.ner_pipeline(chunk)
            # Convert float32 scores to float for JSON serialization
            for entity in chunk_results:
                entity['score'] = float(entity['score'])
            ner_results.append(chunk_results)
        return ner_results

def adjust_token_positions(ner_results, window_start):
    """
    Adjust token positions to be relative to the entire text.

    Args:
        ner_results (List[List[Dict]]): NER results for a window.
        window_start (int): The start position of the window in the entire text.

    Returns:
        List[Dict]: Adjusted NER results with absolute positions.
    """
    adjusted_results = []
    for window in ner_results:
        for entity in window:
            entity['start'] += window_start
            entity['end'] += window_start
            adjusted_results.append(entity)
    return adjusted_results

def merge_entities_across_windows(ner_results, confidence_threshold=0.6):
    """
    Merge entities across overlapping windows.

    Args:
        ner_results (List[Dict]): NER results with absolute positions.
        confidence_threshold (float): Minimum confidence score to consider a prediction.

    Returns:
        List[Dict]: A list of merged entities with summary scores.
    """
    # Sort entities by their start position
    ner_results.sort(key=lambda x: x['start'])

    merged_entities = []
    current_phrase = []
    current_scores = []
    current_start = None
    last_end = None  # Initialize to None to handle the first token correctly

    for entity in ner_results:
        # Check if the entity is part of a "B" or "I" sequence and is continuous
        if entity['entity_group'] in ('B', 'I') and (last_end is None or entity['start'] <= last_end + 1):
            word = entity['word'].strip()
            last_word = current_phrase[-1].strip() if current_phrase else None
            if word.startswith("##"):
                word = word[2:]  # Remove the "##" for subword tokens
            elif current_phrase and not word.startswith(("-", "[", "]", "(", ")", ",", " ")) and (last_word is None or not last_word.endswith(("-", "[", "]", "(", ")", ",", " "))):
                word = " " + word # Add space before new words unless they start with special characters
            current_phrase.append(word)

            current_scores.append(entity['score'])
            last_end = entity['end']
            if current_start is None:
                current_start = entity['start']
        else:
            if current_phrase:
                # Calculate summary score (e.g., average score)
                summary_score = sum(current_scores) / len(current_scores)
                if any(score >= confidence_threshold for score in current_scores):
                    merged_entities.append({
                        'phrase': ''.join(current_phrase),
                        'score': summary_score,
                        'start': current_start,
                        'end': last_end
                    })
                current_phrase = []
                current_scores = []
                current_start = None
                last_end = None

            # Start a new phrase with the current entity
            current_phrase = [entity['word'].strip()]
            current_scores = [entity['score']]
            current_start = entity['start']
            last_end = entity['end']

    # Add the last phrase if it exists
    if current_phrase:
        summary_score = sum(current_scores) / len(current_scores)
        if any(score >= confidence_threshold for score in current_scores):
            merged_entities.append({
                'phrase': ''.join(current_phrase),
                'score': summary_score,
                'start': current_start,
                'end': last_end
            })

    return merged_entities

def parse_tei(file_path: str) -> Dict[str, Any]:
    """
    Parse a TEI XML file to extract relevant information.

    Args:
        file_path (str): The path to the TEI XML file.

    Returns:
        dict: A dictionary containing extracted information.
    """
    # Define the TEI namespace
    ns = {'tei': 'http://www.tei-c.org/ns/1.0'}

    # Parse the XML file
    tree = etree.parse(file_path)
    root = tree.getroot()

    # Helper function to safely extract and strip text
    def safe_findtext(element, path, namespaces):
        text = element.findtext(path, namespaces=namespaces)
        return text.strip() if text else ''

    # Helper function to extract titles with their hierarchy
    def extract_titles_with_hierarchy(element, path=''):
        titles = []
        for title in element.findall('.//tei:title', namespaces=ns):
            title_text = title.text.strip() if title.text else ''
            parent_path = path + '/' + title.tag.split('}')[-1]
            titles.append({'title': title_text, 'path': parent_path})
            # Recursively extract nested titles
            titles.extend(extract_titles_with_hierarchy(title, parent_path))
        return titles

    # Extract titles with hierarchy
    titles_with_hierarchy = extract_titles_with_hierarchy(root)

    # Extract authors
    authors = []
    for author in root.findall('.//tei:fileDesc//tei:author', namespaces=ns):
        pers_name = author.find('.//tei:persName', namespaces=ns)
        if pers_name is not None:
            forename = ' '.join(pers_name.xpath('.//tei:forename/text()', namespaces=ns)).strip()
            surname = safe_findtext(pers_name, 'tei:surname', ns)
            full_name = f"{forename} {surname}".strip()
            email = safe_findtext(author, './/tei:email', ns)
            authors.append({'name': full_name, 'email': email})

    # Extract affiliations
    affiliations = []
    for affiliation in root.findall('.//tei:affiliation', namespaces=ns):
        org_names = [org.text.strip() for org in affiliation.findall('.//tei:orgName', namespaces=ns) if org.text]
        address = ' '.join(' '.join(line.split()) for line in affiliation.xpath('.//tei:address//text()', namespaces=ns)).strip()
        affiliations.append({'org_names': org_names, 'address': address})

    # Extract abstract
    abstract = ' '.join(root.xpath('.//tei:abstract//text()', namespaces=ns)).strip()

    # Extract references with detailed bibliographic information
    references = []
    for ref in root.findall('.//tei:listBibl/tei:biblStruct', namespaces=ns):
        ref_id = ref.get('{http://www.w3.org/XML/1998/namespace}id')
        ref_title = safe_findtext(ref, './/tei:analytic/tei:title', ns)
        ref_authors = []
        for author in ref.findall('.//tei:analytic/tei:author', namespaces=ns):
            pers_name = author.find('.//tei:persName', namespaces=ns)
            if pers_name is not None:
                forename = ' '.join(pers_name.xpath('.//tei:forename/text()', namespaces=ns)).strip()
                surname = safe_findtext(pers_name, 'tei:surname', ns)
                full_name = f"{forename} {surname}".strip()
                ref_authors.append(full_name)
        
        journal_title = safe_findtext(ref, './/tei:monogr/tei:title', ns)
        volume = safe_findtext(ref, './/tei:imprint/tei:biblScope[@unit="volume"]', ns)
        pages = ref.find('.//tei:imprint/tei:biblScope[@unit="page"]', namespaces=ns)
        page_range = f"{pages.get('from')}-{pages.get('to')}" if pages is not None else None
        publication_date = safe_findtext(ref, './/tei:imprint/tei:date[@type="published"]', ns)
        raw_reference = safe_findtext(ref, './/tei:note[@type="raw_reference"]', ns)

        references.append({
            'id': ref_id,
            'title': ref_title,
            'authors': ref_authors,
            'journal': journal_title,
            'volume': volume,
            'pages': page_range,
            'publication_date': publication_date,
            'raw_reference': raw_reference
        })

    # Extract body sections with citation links
    body_sections = []
    for section in root.findall('.//tei:body//tei:div', namespaces=ns):
        section_title = safe_findtext(section, './/tei:head', ns)
        section_text = ' '.join(section.itertext()).strip()
        citations = [ref.get('target') for ref in section.findall('.//tei:ref', namespaces=ns)]
        body_sections.append({'title': section_title, 'text': section_text, 'citations': citations})

    # Extract funding information
    funding_info = []
    funders = {}
    for funder in root.findall('.//tei:funder', namespaces=ns):
        ref_id = funder.get('ref')
        org_name = safe_findtext(funder, './/tei:orgName', ns)
        if ref_id and org_name:
            funders[ref_id] = org_name

    for org in root.findall('.//tei:listOrg/tei:org', namespaces=ns):
        org_id = f"#{org.get('{http://www.w3.org/XML/1998/namespace}id')}"
        grant_number = safe_findtext(org, './/tei:idno[@type="grant-number"]', ns)
        if org_id in funders:
            funding_info.append({'funder': funders[org_id], 'grant_number': grant_number})

    # Extract acknowledgements
    acknowledgements = []
    for ack in root.findall('.//tei:div[@type="acknowledgement"]', namespaces=ns):
        ack_text = ' '.join(ack.itertext()).strip()
        acknowledgements.append(ack_text)

    # Extract tables and figures
    tables = []
    figures = []
    for table in root.findall('.//tei:table', namespaces=ns):
        tables.append(' '.join(table.itertext()).strip())
    for figure in root.findall('.//tei:figure', namespaces=ns):
        figures.append(' '.join(figure.itertext()).strip())

    # Extract publication information
    pub_info = {}
    pub_stmt = root.find('.//tei:publicationStmt', namespaces=ns)
    if pub_stmt is not None:
        pub_info['publisher'] = safe_findtext(pub_stmt, './/tei:publisher', ns)
        pub_info['availability'] = safe_findtext(pub_stmt, './/tei:availability/tei:licence', ns)
        pub_info['published_date'] = safe_findtext(pub_stmt, './/tei:date[@type="published"]', ns)

    source_desc = root.find('.//tei:sourceDesc', namespaces=ns)
    if source_desc is not None:
        monogr = source_desc.find('.//tei:monogr', namespaces=ns)
        if monogr is not None:
            imprint = monogr.find('.//tei:imprint', namespaces=ns)
            if imprint is not None:
                pub_info['published_date'] = safe_findtext(imprint, './/tei:date[@type="published"]', ns)
        pub_info['md5'] = safe_findtext(source_desc, './/tei:idno[@type="MD5"]', ns)
        pub_info['doi'] = safe_findtext(source_desc, './/tei:idno[@type="DOI"]', ns)
        pub_info['submission_note'] = safe_findtext(source_desc, './/tei:note[@type="submission"]', ns)

    return {
        'titles_with_hierarchy': titles_with_hierarchy,
        'authors': authors,
        'affiliations': affiliations,
        'abstract': abstract,
        'references': references,
        'body_sections': body_sections,
        'funding_info': funding_info,
        'acknowledgements': acknowledgements,
        'tables': tables,
        'figures': figures,
        'publication_info': pub_info
    }

def highlight_entities_in_text(all_text, merged_entities):
    """
    Highlight merged entities in the original text for debugging.

    Args:
        all_text (str): The original text.
        merged_entities (List[Dict]): A list of merged entities with their start and end positions.
    """
    # ANSI color codes for terminal output
    highlight_color = "\033[93m"  # Yellow
    reset_color = "\033[0m"

    for entity in merged_entities:
        start = entity['start']
        end = entity['end']
        phrase = entity['phrase']

        # Calculate context boundaries
        context_start = max(0, start - 40)
        context_end = min(len(all_text), end + 40)

        # Print the text with context and highlighted entity
        print(f"Entity: {phrase}, Score: {entity['score']}")
        print(f"{all_text[context_start:start]}{highlight_color}{all_text[start:end]}{reset_color}{all_text[end:context_end]}\n")

def collapse_to_unique_terms(merged_results, score_threshold=0.0, length_threshold=0):
    """
    Collapse merged results into a non-redundant set of unique terms.

    Args:
        merged_results (List[Dict]): A list of merged entities with their phrases and scores.
        score_threshold (float): Minimum score to consider a term.
        length_threshold (int): Minimum length to consider a term.

    Returns:
        List[Dict]: A list of unique terms with the best score.
    """
    unique_terms = {}

    for entity in merged_results:
        phrase = entity['phrase']
        score = entity['score']

        # Apply thresholds
        if score >= score_threshold and len(phrase) >= length_threshold:
            if phrase not in unique_terms or unique_terms[phrase] < score:
                unique_terms[phrase] = score

    # Convert to list of dictionaries
    unique_terms_list = [{'term': term, 'best_score': best_score} for term, best_score in unique_terms.items()]

    return unique_terms_list

def process_file(file_path, ner_models, output_dir):
    """
    Process a single TEI XML file to extract and classify NLP concepts.

    Args:
        file_path (str): The path to the TEI XML file.
        ner_models (List[str]): A list of NER model names to use.
        output_dir (str): The directory where the output JSON file will be saved.
    """
    try:
        # Parse the TEI file
        parsed_data = parse_tei(file_path)

        # Concatenate all text for NER
        all_text = parsed_data.get('abstract', '')
        for section in parsed_data.get('body_sections', []):
            all_text += ' ' + section['text']

        # Process text with each NER model
        for model_name in ner_models:
            ner_pipeline = NERPipeline(model_name=model_name)
            all_ner_results = []
            max_length = 512  # Default to a reasonable chunk size if needed

            for i in range(0, len(all_text), max_length):
                window_text = all_text[i:i+max_length]
                window_results = ner_pipeline.process_text(window_text)
                adjusted_results = adjust_token_positions(window_results, i)
                all_ner_results.extend(adjusted_results)

            merged_results = merge_entities_across_windows(all_ner_results)

            # Debugging: Highlight entities in the original text
            #highlight_entities_in_text(all_text, merged_results)

            # Collapse to unique terms
            unique_terms = collapse_to_unique_terms(merged_results, score_threshold=0.9, length_threshold=3)

            # Add NER results to parsed data
            parsed_data[f'ner_results_{model_name}'] = unique_terms

        # Save the output to a file
        output_file = os.path.join(output_dir, os.path.basename(file_path) + '.json')
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(parsed_data, f, ensure_ascii=False, indent=4)

    except Exception as e:
        print(f"Error processing {file_path}: {e}")
        import traceback
        traceback.print_exc()

def main():
    parser = argparse.ArgumentParser(description="Process TEI XML files and classify NLP concepts.")
    parser.add_argument('output_dir', help="Directory to save the output files.")
    parser.add_argument('file_paths', nargs='*', help="Paths to the TEI XML files. If not provided, read from stdin.")
    parser.add_argument('--workers', type=int, default=1, help="Number of worker processes to use.")

    args = parser.parse_args()

    # Read file paths from stdin if not provided as arguments
    if not args.file_paths:
        args.file_paths = [line.strip() for line in sys.stdin]

    # Ensure output directory exists
    os.makedirs(args.output_dir, exist_ok=True)

    # List of NER models to use
    ner_models = [
        'pruas/BENT-PubMedBERT-NER-Gene',
        'pruas/BENT-PubMedBERT-NER-Bioprocess',
        'pruas/BENT-PubMedBERT-NER-Chemical',
        'pruas/BENT-PubMedBERT-NER-Organism',
        #'ESGBERT/EnvironmentalBERT-environmental'
    ]

    # Use ThreadPoolExecutor to parallelize file processing
    with concurrent.futures.ThreadPoolExecutor(max_workers=args.workers) as executor:
        futures = {executor.submit(process_file, file_path, ner_models, args.output_dir): file_path for file_path in args.file_paths}
        
        # Use tqdm to track progress
        for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc="Processing files"):
            file_path = futures[future]
            try:
                future.result()
            except Exception as e:
                print(f"Error processing {file_path}: {e}")

if __name__ == "__main__":
    main() 